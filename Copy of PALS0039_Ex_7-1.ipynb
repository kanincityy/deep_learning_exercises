{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1x2RcJz5WtHAXFejKdz6Lwj4WtP1lQ6Yw","timestamp":1741014063484},{"file_id":"111x5PYdoid6snqPORc7o4t1MS8QmfRC0","timestamp":1645972524180},{"file_id":"https://github.com/mhuckvale/pals0039/blob/master/Demo_LM.ipynb","timestamp":1645806246579}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"-QYlAUr-y-kX"},"source":["[![PALS0039 Logo](https://www.phon.ucl.ac.uk/courses/pals0039/images/pals0039logo.png)](https://www.phon.ucl.ac.uk/courses/pals0039/)\n","\n","# Exercise 7.1 N-gram language modelling using NLTK\n","\n","In this exercise we experiment with n-gram language models using [`NLTK`'s functionality](https://www.nltk.org/api/nltk.lm.html).\n","\n","You might also find the following article insightful: [Language Modeling with NLTK](https://medium.com/swlh/language-modelling-with-nltk-20eac7e70853)\n","\n"]},{"cell_type":"code","metadata":{"id":"znfr9EdrPcoV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741108562164,"user_tz":0,"elapsed":12879,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}},"outputId":"1576ace0-93fd-4016-804f-291ae0b8da6a"},"source":["!pip install -U nltk>=3.7.0\n","\n","import nltk\n","nltk.download(\"reuters\")\n","nltk.download(\"punkt_tab\")\n","from nltk.corpus import reuters\n","\n","from nltk.lm import Vocabulary\n","from nltk.util import pad_sequence, ngrams\n","from nltk.lm.preprocessing import flatten\n","from nltk.lm.models import Laplace"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package reuters to /root/nltk_data...\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]}]},{"cell_type":"markdown","source":["(a) ***Collect*** all the sentences from the `reuters` corpus, ***lowercase*** them, and ***pad*** the start and end with special symbols (this means we will have n-grams that distinguish the start and end of sentences). For the left pad symbol use `<s>` and the right use `</s>`.\n","\n","Hint: Use the [`reuters.sents()`](https://www.nltk.org/api/nltk.corpus.html#corpus-reader-functions) method and the `pad_sequence` function that have already been imported"],"metadata":{"id":"YEv1-ttOkOLB"}},{"cell_type":"code","source":["#(a)\n","sentences = []\n","# Corpus reader function that takes in some kind of input and read it\n","# item argument shows which doc from corpus should be read\n","# .sents() = means that we are looking at list of list of str\n","for s in reuters.sents():\n","  lower_s = [word.lower() for word in s]\n","  # We are iterating through the corpus, splitting each sentence into words, lowering those words, and padding each sentence\n","  padded_lower_s = list(pad_sequence(lower_s,\n","                                     n=2,\n","                                     pad_left=True,\n","                                     left_pad_symbol=\"<s>\",\n","                                     pad_right=True,\n","                                     right_pad_symbol=\"</s>\"))\n","  sentences.append(padded_lower_s)\n","\n","#Inspect the first 3 sentences\n","for s in sentences[:3]:\n","  print(s)"],"metadata":{"id":"CSvb1BYkjXJl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741108573229,"user_tz":0,"elapsed":11057,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}},"outputId":"64694f7b-5429-42bf-ebe4-c948c56a1f7e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["['<s>', 'asian', 'exporters', 'fear', 'damage', 'from', 'u', '.', 's', '.-', 'japan', 'rift', 'mounting', 'trade', 'friction', 'between', 'the', 'u', '.', 's', '.', 'and', 'japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'asia', \"'\", 's', 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'far', '-', 'reaching', 'economic', 'damage', ',', 'businessmen', 'and', 'officials', 'said', '.', '</s>']\n","['<s>', 'they', 'told', 'reuter', 'correspondents', 'in', 'asian', 'capitals', 'a', 'u', '.', 's', '.', 'move', 'against', 'japan', 'might', 'boost', 'protectionist', 'sentiment', 'in', 'the', 'u', '.', 's', '.', 'and', 'lead', 'to', 'curbs', 'on', 'american', 'imports', 'of', 'their', 'products', '.', '</s>']\n","['<s>', 'but', 'some', 'exporters', 'said', 'that', 'while', 'the', 'conflict', 'would', 'hurt', 'them', 'in', 'the', 'long', '-', 'run', ',', 'in', 'the', 'short', '-', 'term', 'tokyo', \"'\", 's', 'loss', 'might', 'be', 'their', 'gain', '.', '</s>']\n"]}]},{"cell_type":"markdown","source":["(b) If needed, ***flatten*** the sentences into a single list of words representing the entire corpus and create a finite vocabulary using NLTK's [`Vocabulary` constructor](https://www.nltk.org/api/nltk.lm.html#nltk.lm.Vocabulary) by specifying a frequency cut-off at 10.\n","\n","(c) Subsequently, inspect the lengths of the corpus and the vocabulary. Compare the length of the vocabulary with the number of unique words in the corpus."],"metadata":{"id":"AgzFpOs6pFWC"}},{"cell_type":"code","source":["#(b)\n","import itertools\n","# Create vocabulary of corpus\n","# unk_cutoff (int) â€“ Words that occur less frequently than this value are not considered part of the vocabulary.\n","flat_sentences = list(flatten(sentences))\n","vocab = Vocabulary(flat_sentences, unk_cutoff=10)\n","\n","#(c)\n","print(f\"Number of tokens in corpus: {len(flat_sentences)}\")\n","print(f\"Number of unique tokens: {len(vocab)}\")"],"metadata":{"id":"hwOuOGE3hxGC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741109991234,"user_tz":0,"elapsed":258,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}},"outputId":"fb25b610-9988-45a1-989a-143254774d74"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of tokens in corpus: 1830349\n","Number of unique tokens: 8070\n"]}]},{"cell_type":"markdown","source":["(d) Split the text into `train` and `test` sets as follows: reserve the first 10,000 words for the `test` set and the rest for `train`."],"metadata":{"id":"KHmjksbV6WXK"}},{"cell_type":"code","source":["#(d)\n","train_words = flat_sentences[10000:]\n","test = flat_sentences[:10000]"],"metadata":{"id":"80iggfyP6mAV","executionInfo":{"status":"ok","timestamp":1741108573558,"user_tz":0,"elapsed":22,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["Train three n-gram language models with n=1,2,3 respectively. Use add-one smoothing (using NLTK's [`Laplace` constructor]())"],"metadata":{"id":"tEV19XNn6w6Q"}},{"cell_type":"code","source":["lms = {}\n","\n","for n in [1, 2, 3]:\n","  train_ngrams = list(ngrams(train_words, n))\n","  print(train_ngrams[:10])\n","  # Laplace constructor = Implements Laplace (add one) smoothing.\n","  lm = Laplace(n)\n","  lm.fit([train_ngrams], vocab)\n","  lms[n] = lm"],"metadata":{"id":"iguvnMrX8kg8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741108607110,"user_tz":0,"elapsed":33457,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}},"outputId":"f53b60bb-e118-452d-864a-a1f7493e42df"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[('reasons',), ('are',), ('low',), ('domestic',), ('inflation',), (',',), ('a',), ('bottoming',), ('out',), ('of',)]\n","[('reasons', 'are'), ('are', 'low'), ('low', 'domestic'), ('domestic', 'inflation'), ('inflation', ','), (',', 'a'), ('a', 'bottoming'), ('bottoming', 'out'), ('out', 'of'), ('of', 'the')]\n","[('reasons', 'are', 'low'), ('are', 'low', 'domestic'), ('low', 'domestic', 'inflation'), ('domestic', 'inflation', ','), ('inflation', ',', 'a'), (',', 'a', 'bottoming'), ('a', 'bottoming', 'out'), ('bottoming', 'out', 'of'), ('out', 'of', 'the'), ('of', 'the', 'fall')]\n"]}]},{"cell_type":"markdown","source":["(e) Evaluate each of the 3 language models by determining the perplexity on the `train` and `test` sets.\n","\n","Hint: Use the [`perplexity` method](https://www.nltk.org/api/nltk.lm.api.html#nltk.lm.api.LanguageModel.perplexity)"],"metadata":{"id":"D_RbK2MB9lcD"}},{"cell_type":"code","source":["#(e)\n","from nltk.lm import Laplace\n","from nltk.util import ngrams\n","from nltk.lm.preprocessing import padded_everygram_pipeline\n","\n","# Initialize an empty dictionary to store the models\n","lms = {}\n","\n","# Loop over n-grams (1, 2, 3)\n","for n in [1, 2, 3]:\n","    # Prepare the n-grams and vocabulary\n","    train_ngrams, vocab = padded_everygram_pipeline(n, train_words)\n","\n","    # Initialize Laplace model with smoothing\n","    lm = Laplace(n)  # Laplace smoothing\n","\n","    # Fit the language model on the training n-grams\n","    lm.fit(train_ngrams, vocab)\n","\n","    # Store the trained model in the dictionary\n","    lms[n] = lm\n","\n","    # Calculate and print perplexity for the train and test sets\n","    train_perplexity = lm.perplexity(train_words)\n","    test_perplexity = lm.perplexity(test)\n","\n","    print(f\"Perplexity of train words for {n}-gram model: {train_perplexity}\")\n","    print(f\"Perplexity of test words for {n}-gram model: {test_perplexity}\")"],"metadata":{"id":"MWG6ePz79l0x","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741109043770,"user_tz":0,"elapsed":256517,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}},"outputId":"c70a7652-f15c-402f-daaf-1279d0f80cb9"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Perplexity of train words for 1-gram model: 68.48512959427785\n","Perplexity of test words for 1-gram model: 66.23824742683821\n","Perplexity of train words for 2-gram model: 75.73408595163743\n","Perplexity of test words for 2-gram model: 72.99160518233431\n","Perplexity of train words for 3-gram model: 79.84740979500924\n","Perplexity of test words for 3-gram model: 76.74789630733954\n"]}]},{"cell_type":"markdown","source":["(f) Comment on the performance of the three models, which one is best. Why?"],"metadata":{"id":"8fkApeNVAar3"}},{"cell_type":"code","source":["#(f)\n","# Perplexity shows how confident the model is in assigning probabilities to its predictions\n","# A lower perplexity means that the model assigned higher probabilities to chosen sequence of generated words, which means better performance\n","# Could argue that perplexity cannot be a representative value based on different architecture of these models"],"metadata":{"id":"V2MWn7tYAkVZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(g) What would the perplexity be for a predictor which randomly guesses from any one of the words occurring in the test set?"],"metadata":{"id":"VXmPI1orBpf_"}},{"cell_type":"code","source":["#(g)\n","# Size of vocabulary of test set = 8070"],"metadata":{"id":"Q_rJeobSBzqn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(h) **Optional:** Experiment with models using different smoothing approaches. What is the best perplexity you can achieve?"],"metadata":{"id":"yrMeS72JCLgQ"}}]}